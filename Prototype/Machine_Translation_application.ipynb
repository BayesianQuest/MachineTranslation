{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Translation_application.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-vYkIM0t9vV"
      },
      "source": [
        "### Machine Translation Prototype\n",
        "\n",
        "This is the prototype for the machine translation model we are going to build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-5RrIwSCYjt",
        "outputId": "9e37d53e-5e88-44f9-eaae-defed058c156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY8WdPszCify"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import numpy as np\n",
        "from numpy.random import shuffle\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWKWFKYwBVl_"
      },
      "source": [
        "# Defining the path to the raw data set\n",
        "fileurl = '/content/drive/My Drive/Bayesian Quest/deu.txt'\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3RLC0k_G-we"
      },
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "    # open the file\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    \n",
        "    # Split the text into individual lines\n",
        "    lines = text.strip().split('\\n')\n",
        "    # Splitting each line based on tab spaces and creating a list\n",
        "    lines = [line.split('\\t') for line in lines]\n",
        "\n",
        "    file.close()\n",
        "    return array(lines)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVELXcIlS5t2",
        "outputId": "32084165-9317-4a77-f5f6-1a9d9afb6082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# Reading the data using the function\n",
        "mtData = read_text(fileurl)\n",
        "# Taking only 50000 rows of data\n",
        "mtData = mtData[:50000,:2]\n",
        "print(mtData.shape)\n",
        "mtData[0:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Go.', 'Geh.'],\n",
              "       ['Hi.', 'Hallo!'],\n",
              "       ['Hi.', 'Grüß Gott!'],\n",
              "       ['Run!', 'Lauf!'],\n",
              "       ['Run.', 'Lauf!'],\n",
              "       ['Wow!', 'Potzdonner!'],\n",
              "       ['Wow!', 'Donnerwetter!'],\n",
              "       ['Fire!', 'Feuer!'],\n",
              "       ['Help!', 'Hilfe!'],\n",
              "       ['Help!', 'Zu Hülf!']], dtype='<U537')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLZ6_SrSYqzf"
      },
      "source": [
        "Removing all unwanted characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD1roLD4HQEl"
      },
      "source": [
        "# Cleaning the document from all unwanted characters\n",
        "\n",
        "def cleanDocs(lines):\n",
        "  cleanArray = list()\n",
        "  for docs in lines:\n",
        "    cleanDocs = list()\n",
        "    for line in docs:\n",
        "      # Normalising unicode characters\n",
        "      line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "      line = line.decode('UTF-8')\n",
        "      # Tokenize on white space\n",
        "      line = line.split()\n",
        "      # Removing punctuations from each token\n",
        "      line = [word.translate(str.maketrans('', '', string.punctuation)) for word in line]\n",
        "      # convert to lower case\n",
        "      line = [word.lower() for word in line]\n",
        "      # Remove tokens with numbers in them\n",
        "      line = [word for word in line if word.isalpha()]\n",
        "      # Store as string\n",
        "      cleanDocs.append(' '.join(line))\n",
        "    cleanArray.append(cleanDocs)\n",
        "  return array(cleanArray)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsGpooAwfdjZ",
        "outputId": "521e8c6b-1a45-41e5-b256-3077b22f21e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "# Cleaning the sentences\n",
        "cleanMtDocs = cleanDocs(mtData)\n",
        "cleanMtDocs[0:10]\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['go', 'geh'],\n",
              "       ['hi', 'hallo'],\n",
              "       ['hi', 'gru gott'],\n",
              "       ['run', 'lauf'],\n",
              "       ['run', 'lauf'],\n",
              "       ['wow', 'potzdonner'],\n",
              "       ['wow', 'donnerwetter'],\n",
              "       ['fire', 'feuer'],\n",
              "       ['help', 'hilfe'],\n",
              "       ['help', 'zu hulf']], dtype='<U117')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvsqB3372Lfj",
        "outputId": "6779c7c1-9c33-484b-d5d3-fe11519336ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The dimensions of the data set\n",
        "len(cleanMtDocs)\n",
        "print(cleanMtDocs.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDJQe8YU2Q7L",
        "outputId": "74ee0fdb-5d0e-4a22-aa6f-cbe943eaa36e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "# Shuffling the data\n",
        "shuffle(cleanMtDocs)\n",
        "cleanMtDocs[0:10]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['i admire your talent', 'ich bewundere dein talent'],\n",
              "       ['i feel strong', 'ich fuhle mich stark'],\n",
              "       ['i need toms help', 'ich brauche toms hilfe'],\n",
              "       ['how is that spelled', 'wie wird das buchstabiert'],\n",
              "       ['what is this for', 'wofur ist das'],\n",
              "       ['tomll remember', 'tom wird sich daran erinnern'],\n",
              "       ['give him time', 'gib ihm zeit'],\n",
              "       ['did you see tom leave', 'haben sie tom gehen sehen'],\n",
              "       ['well walk', 'wir werden zu fu gehen'],\n",
              "       ['i wont pay this bill', 'ich bezahle diese rechnung nicht']],\n",
              "      dtype='<U117')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrlFcRI26UrY"
      },
      "source": [
        "### Starting the Neural Translation Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIuHGqWO4zvR"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTjDeXAG6qPH"
      },
      "source": [
        "# Creating the tokenizers\n",
        "# Function for creating tokenizers\n",
        "def createTokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1w6PPvG-mcp",
        "outputId": "4c1f0a97-bb04-4afd-cad3-026c01c0a59e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create English Tokenizer\n",
        "eng_tokenizer = createTokenizer(cleanMtDocs[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "print('Length of english vocabulary',eng_vocab_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of english vocabulary 6255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDJkGeC1Fi9I",
        "outputId": "7f49d202-5d31-4c1a-d610-b76c70edcbc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "# Listing the first 10 items of the English tokenizer\n",
        "list(eng_tokenizer.word_index.items())[0:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tom', 1),\n",
              " ('i', 2),\n",
              " ('you', 3),\n",
              " ('is', 4),\n",
              " ('a', 5),\n",
              " ('it', 6),\n",
              " ('the', 7),\n",
              " ('to', 8),\n",
              " ('me', 9),\n",
              " ('im', 10)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzvk2fpo2Qau",
        "outputId": "4ef5a6e4-efb9-4e3e-e85e-42e4445588bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create German tokenizer\n",
        "ger_tokenizer = createTokenizer(cleanMtDocs[:,1])\n",
        "# Defining German Vocabulary\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "print(ger_vocab_size)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv_tZFEz2qYX"
      },
      "source": [
        "### Finding the optimum length for the German and English vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfJmw7kk2wTY",
        "outputId": "5a966a81-1bb1-4c00-afce-542a37b3b68a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create an empty list to store all english sentence lenghts\n",
        "len_english = []\n",
        "# Getting the length of all the English sentences\n",
        "[len_english.append(len(line.split())) for line in cleanMtDocs[:,0]]\n",
        "len_english[0:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 3, 4, 4, 4, 2, 3, 5, 2, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkaWpE1r4DLI",
        "outputId": "aa0ef795-7c0a-4182-a1d7-b4731ebf7296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len_German = []\n",
        "# Getting the length of all the English sentences\n",
        "[len_German.append(len(line.split())) for line in cleanMtDocs[:,1]]\n",
        "len_German[0:10]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4, 4, 4, 4, 3, 5, 3, 5, 5, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En045tHI4T-B"
      },
      "source": [
        "##### Finding the optimimum sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPDpSiM13Jbr",
        "outputId": "790b9309-d433-4a8a-f0ff-bec20241912a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find the quantile length\n",
        "engLength = np.quantile(len_english, .975)\n",
        "engLength"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4oG1nAj4sbW",
        "outputId": "183ca0bf-1533-4e2c-d3d2-d7a667bb4140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Find the quantile length\n",
        "gerLength = np.quantile(len_German, .975)\n",
        "gerLength"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXDPPew77OkP"
      },
      "source": [
        "### Encoding the sequences \n",
        "\n",
        "In this phase we will encode each of the sentences as integers in a sequence. Another task which needs to be done is to ensure that the lengths are standard. This is the reason we calcualated the maximum length of each sequence. We get the lengths standard by zero padding the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33gmNLZC6Xh2"
      },
      "source": [
        "# Function for encoding and padding sequences\n",
        "\n",
        "def encode_sequences(tokenizer,length, lines):\n",
        "    # Sequences as integers\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # Padding the sentences with 0\n",
        "    X = pad_sequences(X,maxlen=length,padding='post')\n",
        "    return X"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFBqmIms7ebc",
        "outputId": "ce6a33a7-6327-4f69-9597-1d0e0a08d91f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Preparing the train and test splits\n",
        "from sklearn.model_selection import train_test_split\n",
        "# split data into train and test set\n",
        "train, test = train_test_split(cleanMtDocs, test_size=0.1, random_state = 123)\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 2)\n",
            "(5000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScRm9223Ah2V",
        "outputId": "03df43e3-79b2-49d2-ed06-3d03eeaa4c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Creating the X variable for both train and test sets\n",
        "trainX = encode_sequences(ger_tokenizer,int(gerLength),train[:,1])\n",
        "testX = encode_sequences(ger_tokenizer,int(gerLength),test[:,1])\n",
        "print(trainX.shape)\n",
        "print(testX.shape)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 6)\n",
            "(5000, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZij3DQKP70V",
        "outputId": "57f43d6e-6631-4bdc-eca1-5e3014af0378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Displaying first 5 rows of the traininig set\n",
        "trainX[0:5]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  82,   25,    1,  356,    0,    0],\n",
              "       [  10,   19,  670,    0,    0,    0],\n",
              "       [   5,  600, 1113,    0,    0,    0],\n",
              "       [   2,  111,  523,    0,    0,    0],\n",
              "       [  90,   29,   14,  413,  134,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdYkTaz4Hgga",
        "outputId": "f48cb580-b97b-4ab4-d89e-289527518866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Creating the Y variable both train and test\n",
        "trainY = encode_sequences(eng_tokenizer,int(engLength),train[:,0])\n",
        "testY = encode_sequences(eng_tokenizer,int(engLength),test[:,0])\n",
        "print(trainY.shape)\n",
        "print(testY.shape)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(45000, 5)\n",
            "(5000, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYogm7VjKLqT"
      },
      "source": [
        "### Modelling "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiEB8iphKN1R"
      },
      "source": [
        "def defineModel(src_vocab,tar_vocab,src_timesteps,tar_timesteps,n_units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(src_vocab,n_units,input_length=src_timesteps,mask_zero=True))\n",
        "    model.add(LSTM(n_units))\n",
        "    model.add(RepeatVector(tar_timesteps))\n",
        "    model.add(LSTM(n_units,return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(tar_vocab,activation='softmax')))\n",
        "    # Compiling the model\n",
        "    model.compile(optimizer = 'adam',loss='sparse_categorical_crossentropy')\n",
        "    # Summarising the model\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_StmYsA1KnoX",
        "outputId": "7b79df2a-499b-4522-f7df-4fba99f49dde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "model = defineModel(ger_vocab_size,eng_vocab_size,int(gerLength),int(engLength),256)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 6, 256)            2613760   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 5, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 5, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 5, 6255)           1607535   \n",
            "=================================================================\n",
            "Total params: 5,271,919\n",
            "Trainable params: 5,271,919\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0UD_4YwLyX9"
      },
      "source": [
        "# Fitting the model\n",
        "checkpoint = ModelCheckpoint('model1.h5',monitor='val_loss',verbose=1,save_best_only=True,mode='min')\n",
        "model.fit(trainX,trainY,epochs=50,batch_size=64,validation_data=(testX,testY),callbacks=[checkpoint],verbose=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtAC_t-1YU6S"
      },
      "source": [
        "# loading the model from the best model saved\n",
        "model = load_model('/content/drive/My Drive/Bayesian Quest/model1.h5')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfLC2Xju876d"
      },
      "source": [
        "#### Predictions with the model on the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9URS0oVVM2d-",
        "outputId": "58446d63-e46f-4ac1-83d3-ed0e46aacd96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Generating the predictions\n",
        "prediction = model.predict(testX,verbose=0)\n",
        "prediction.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 5, 6255)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBVoPAWu9BM_",
        "outputId": "3ce3e3de-6670-4670-9bcd-1bf3be409a6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 5, 6255)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fNZj1e69P06",
        "outputId": "a7bb41ab-bcab-4062-adcf-9039f81c79bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Getting the prediction index along the last axis ( Vocabulary size axis)\n",
        "predIndex = [argmax(vector,axis = -1) for vector in prediction]\n",
        "predIndex[0:3]"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([   5,  123,    4, 3052,    0]),\n",
              " array([  2,  14,  47,   7, 383]),\n",
              " array([  1, 476, 356,   0,   0])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPp3tm40-DZt"
      },
      "source": [
        "# Creating the reverse dictionary\n",
        "reverse_eng = eng_tokenizer.index_word\n"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQMLNkiC-V5o",
        "outputId": "a2e69976-1685-4218-cc96-2181dec76d14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Converting the tokens to a sentence\n",
        "preds = []\n",
        "for pred in predIndex[0]:\n",
        "  if pred == 0:\n",
        "        continue \n",
        "  preds.append(reverse_eng[pred])  \n",
        "print(' '.join(preds))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a dog is barking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWb_oLU4-uYG",
        "outputId": "004a2d21-3c74-4838-dce2-8e1a71129e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Looking at the target sentence\n",
        "preds = []\n",
        "for pred in testY[0]:\n",
        "  if pred == 0:\n",
        "        continue \n",
        "  preds.append(reverse_eng[pred])  \n",
        "print(' '.join(preds))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a dog is barking\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0sLnoe0BjON"
      },
      "source": [
        "# Creating a function for converting sequences\n",
        "def Convertsequence(tokenizer,source):\n",
        "    target = list()\n",
        "    reverse_eng = tokenizer.index_word\n",
        "    for i in source:\n",
        "        if i == 0:\n",
        "            continue\n",
        "        target.append(reverse_eng[int(i)])\n",
        "    return ' '.join(target)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHWKChG7EMn5"
      },
      "source": [
        "# Function to generate predictions from source data\n",
        "def generatePredictions(model,tokenizer,data):\n",
        "    prediction = model.predict(data,verbose=0)\n",
        "    AllPreds = []\n",
        "    for i in range(len(prediction)):\n",
        "        predIndex = [argmax(prediction[i, :, :], axis=-1)][0]\n",
        "        target = Convertsequence(tokenizer,predIndex)\n",
        "        AllPreds.append(target)\n",
        "    return AllPreds"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDKKValVEk9i"
      },
      "source": [
        "# Generate predictions\n",
        "predSent = generatePredictions(model,eng_tokenizer,testX[0:20,:])"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71wFlufaEs1Q",
        "outputId": "5ff010c0-5d01-4868-bb59-4c687cac764f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "for i in range(len(testY[0:20])):\n",
        "    targetY = Convertsequence(eng_tokenizer,testY[i:i+1][0])\n",
        "    print(\"Original sentence : {} :: Prediction : {}\".format([targetY],[predSent[i]]))"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence : ['a dog is barking'] :: Prediction : ['a dog is barking']\n",
            "Original sentence : ['ive been to the mall'] :: Prediction : ['i was at the first']\n",
            "Original sentence : ['tom sounds mad'] :: Prediction : ['tom sounds crazy']\n",
            "Original sentence : ['he must be over sixty'] :: Prediction : ['he must be over sixty']\n",
            "Original sentence : ['freeze'] :: Prediction : ['stop around']\n",
            "Original sentence : ['they feel hungry'] :: Prediction : ['youre hungry']\n",
            "Original sentence : ['tom wants an apple'] :: Prediction : ['tom wants an an']\n",
            "Original sentence : ['im new'] :: Prediction : ['im new']\n",
            "Original sentence : ['i woke you up'] :: Prediction : ['i woke her her']\n",
            "Original sentence : ['are you watching me'] :: Prediction : ['do you understand me']\n",
            "Original sentence : ['what does tom have on'] :: Prediction : ['what tom got']\n",
            "Original sentence : ['tom was pretty bummed'] :: Prediction : ['i was almost']\n",
            "Original sentence : ['see what i mean'] :: Prediction : ['do i order my leap']\n",
            "Original sentence : ['we want it back'] :: Prediction : ['we want it get it']\n",
            "Original sentence : ['nothing gets past you'] :: Prediction : ['nothing gets past you']\n",
            "Original sentence : ['you look very happy'] :: Prediction : ['you look very happy']\n",
            "Original sentence : ['tom was assaulted'] :: Prediction : ['tom was attacked']\n",
            "Original sentence : ['you may see tom there'] :: Prediction : ['you must see tom']\n",
            "Original sentence : ['the radio is on'] :: Prediction : ['the radio is broken']\n",
            "Original sentence : ['tom saved our lives'] :: Prediction : ['tom saved us life']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ3jWO64aGb-"
      },
      "source": [
        "### Predicting on your own sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Ba59WyaE7R"
      },
      "source": [
        "def cleanInput(lines):\n",
        "    cleanSent = []\n",
        "    cleanDocs = list()\n",
        "    for docs in lines.split():\n",
        "        line = normalize('NFD', docs).encode('ascii', 'ignore')\n",
        "        line = line.decode('UTF-8')\n",
        "        line = [line.translate(str.maketrans('', '', string.punctuation))]\n",
        "        line = line[0].lower()\n",
        "        cleanDocs.append(line)\n",
        "    cleanSent.append(' '.join(cleanDocs))\n",
        "    return array(cleanSent)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNR67pQJsegE"
      },
      "source": [
        "# Trying different input sentences\n",
        "inputSentence = 'Es ist ein großartiger Tag' # It is a great day ?\n",
        "#inputSentence ='Heute wird es regnen' #  it's going to rain Today\n",
        "#inputSentence ='Ich habe im Radio gesprochen' # I spoke on the radio"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqz9OTDGatas",
        "outputId": "20f80dda-7d13-40a5-f456-dc0946f36501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "# Clean the input sentence\n",
        "cleanText = cleanInput(inputSentence)\n",
        "cleanText"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['es ist ein groartiger tag'], dtype='<U25')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6jSeId6bVnX",
        "outputId": "d2ce4007-709f-4e7e-c083-cc90defb31b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Encode the inputsentence as sequence of integers\n",
        "seq1 = encode_sequences(ger_tokenizer,int(gerLength),cleanText)\n",
        "seq1"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   7,    3,   12, 2314,  196,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGpfWJ2gc_qm",
        "outputId": "2948d576-7946-49be-bd2a-d6d7918dbb04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Generate the prediction\n",
        "predSent = generatePredictions(model,eng_tokenizer,seq1)\n",
        "\n",
        "print(\"Original sentence : {} :: Prediction : {}\".format([cleanText[0]],predSent))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence : ['es ist ein groartiger tag'] :: Prediction : ['its still ok']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2oT5_zrGMmn",
        "outputId": "987ef923-591a-4b08-e03a-c4ba33e5b6db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "inputSentence1 ='Heute wird es regnen' #  it's going to rain Today\n",
        "inputSentence2 ='Ich habe im Radio gesprochen' # I spoke on the radio\n",
        "\n",
        "for sentence in [inputSentence1,inputSentence2]:\n",
        "  cleanText = cleanInput(sentence)\n",
        "  seq1 = encode_sequences(ger_tokenizer,int(gerLength),cleanText)\n",
        "  # Generate the prediction\n",
        "  predSent = generatePredictions(model,eng_tokenizer,seq1)\n",
        "\n",
        "  print(\"Original sentence : {} :: Prediction : {}\".format([cleanText[0]],predSent))\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sentence : ['heute wird es regnen'] :: Prediction : ['it be in today']\n",
            "Original sentence : ['ich habe im radio gesprochen'] :: Prediction : ['i have your cards']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}